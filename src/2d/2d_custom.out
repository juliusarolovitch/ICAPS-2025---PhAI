Using device: cuda
Loaded normalization values from /home/julius/Desktop/icra_phai/src/2d/normalization_values/vanilla_normalization_values.pkl
Loaded dataset from /home/julius/Desktop/icra_phai/src/2d/datasets/vanilla_dataset.pkl
Training samples: 4429684, Validation samples: 1107422
Calculated input size for MLP: 520
Training MLPModel with learning type: priority
Training model with loss function: custom
Training Epochs:   0%|          | 0/30 [00:00<?, ?it/s]/home/julius/.local/lib/python3.8/site-packages/torch/autograd/graph.py:769: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Training Epochs:   3%|▎         | 1/30 [17:59<8:41:41, 1079.38s/it]Epoch: 1/30, Train Loss: 0.105832, MSE Loss: 0.005828, Grad1 Loss: 0.000049, Grad2 Loss: 0.000024, Grad4 Loss: 2.000000, Val Loss: 0.003439, LR: 0.001
    Current Lambdas: lambda1=0.0500, lambda2=0.0500, lambda3=0.0500
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:   7%|▋         | 2/30 [58:00<14:26:38, 1857.10s/it]Epoch: 2/30, Train Loss: 0.105721, MSE Loss: 0.003936, Grad1 Loss: 0.000029, Grad2 Loss: 0.000012, Grad4 Loss: 2.000000, Val Loss: 0.002925, LR: 0.001
    Current Lambdas: lambda1=0.0509, lambda2=0.0509, lambda3=0.0509
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  10%|█         | 3/30 [1:30:50<14:18:48, 1908.46s/it]Epoch: 3/30, Train Loss: 0.110781, MSE Loss: 0.003645, Grad1 Loss: 0.000026, Grad2 Loss: 0.000011, Grad4 Loss: 2.000000, Val Loss: 0.002485, LR: 0.001
    Current Lambdas: lambda1=0.0536, lambda2=0.0536, lambda3=0.0536
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  13%|█▎        | 4/30 [1:56:13<12:41:04, 1756.32s/it]Epoch: 4/30, Train Loss: 0.119530, MSE Loss: 0.003475, Grad1 Loss: 0.000023, Grad2 Loss: 0.000010, Grad4 Loss: 2.000000, Val Loss: 0.002318, LR: 0.001
    Current Lambdas: lambda1=0.0580, lambda2=0.0580, lambda3=0.0580
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  17%|█▋        | 5/30 [2:22:08<11:41:32, 1683.70s/it]Epoch: 5/30, Train Loss: 0.131886, MSE Loss: 0.003347, Grad1 Loss: 0.000021, Grad2 Loss: 0.000009, Grad4 Loss: 2.000000, Val Loss: 0.002434, LR: 0.001
    Current Lambdas: lambda1=0.0643, lambda2=0.0643, lambda3=0.0643
Training Epochs:  20%|██        | 6/30 [2:48:23<10:58:44, 1646.86s/it]Epoch: 6/30, Train Loss: 0.147836, MSE Loss: 0.003244, Grad1 Loss: 0.000019, Grad2 Loss: 0.000008, Grad4 Loss: 2.000000, Val Loss: 0.002096, LR: 0.001
    Current Lambdas: lambda1=0.0723, lambda2=0.0723, lambda3=0.0723
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  23%|██▎       | 7/30 [3:16:57<10:39:39, 1668.68s/it]Epoch: 7/30, Train Loss: 0.167372, MSE Loss: 0.003161, Grad1 Loss: 0.000017, Grad2 Loss: 0.000007, Grad4 Loss: 2.000000, Val Loss: 0.002109, LR: 0.001
    Current Lambdas: lambda1=0.0821, lambda2=0.0821, lambda3=0.0821
Training Epochs:  27%|██▋       | 8/30 [3:41:50<9:51:18, 1612.64s/it] Epoch: 8/30, Train Loss: 0.190490, MSE Loss: 0.003092, Grad1 Loss: 0.000015, Grad2 Loss: 0.000007, Grad4 Loss: 2.000000, Val Loss: 0.001974, LR: 0.001
    Current Lambdas: lambda1=0.0937, lambda2=0.0937, lambda3=0.0937
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  30%|███       | 9/30 [4:07:30<9:16:31, 1590.09s/it]Epoch: 9/30, Train Loss: 0.217183, MSE Loss: 0.003031, Grad1 Loss: 0.000014, Grad2 Loss: 0.000006, Grad4 Loss: 2.000000, Val Loss: 0.001888, LR: 0.001
    Current Lambdas: lambda1=0.1071, lambda2=0.1071, lambda3=0.1071
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  33%|███▎      | 10/30 [4:24:57<7:54:05, 1422.29s/it]Epoch: 10/30, Train Loss: 0.247449, MSE Loss: 0.002976, Grad1 Loss: 0.000012, Grad2 Loss: 0.000006, Grad4 Loss: 2.000000, Val Loss: 0.002044, LR: 0.001
    Current Lambdas: lambda1=0.1222, lambda2=0.1222, lambda3=0.1222
Training Epochs:  37%|███▋      | 11/30 [4:36:42<6:20:53, 1202.81s/it]Epoch: 11/30, Train Loss: 0.281298, MSE Loss: 0.002936, Grad1 Loss: 0.000011, Grad2 Loss: 0.000005, Grad4 Loss: 2.000000, Val Loss: 0.002043, LR: 0.001
    Current Lambdas: lambda1=0.1392, lambda2=0.1392, lambda3=0.1392
Training Epochs:  40%|████      | 12/30 [4:48:54<5:17:54, 1059.71s/it]Epoch: 12/30, Train Loss: 0.318711, MSE Loss: 0.002894, Grad1 Loss: 0.000010, Grad2 Loss: 0.000005, Grad4 Loss: 2.000000, Val Loss: 0.001980, LR: 0.001
    Current Lambdas: lambda1=0.1579, lambda2=0.1579, lambda3=0.1579
Training Epochs:  43%|████▎     | 13/30 [5:01:05<4:32:00, 960.04s/it] Epoch: 13/30, Train Loss: 0.359695, MSE Loss: 0.002855, Grad1 Loss: 0.000009, Grad2 Loss: 0.000005, Grad4 Loss: 2.000000, Val Loss: 0.001812, LR: 0.001
    Current Lambdas: lambda1=0.1784, lambda2=0.1784, lambda3=0.1784
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  47%|████▋     | 14/30 [5:13:12<3:57:14, 889.67s/it]Epoch: 14/30, Train Loss: 0.404254, MSE Loss: 0.002824, Grad1 Loss: 0.000008, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001698, LR: 0.001
    Current Lambdas: lambda1=0.2007, lambda2=0.2007, lambda3=0.2007
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  50%|█████     | 15/30 [5:24:51<3:28:03, 832.20s/it]Epoch: 15/30, Train Loss: 0.452378, MSE Loss: 0.002792, Grad1 Loss: 0.000007, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001836, LR: 0.001
    Current Lambdas: lambda1=0.2248, lambda2=0.2248, lambda3=0.2248
Training Epochs:  53%|█████▎    | 16/30 [5:37:11<3:07:41, 804.36s/it]Epoch: 16/30, Train Loss: 0.504074, MSE Loss: 0.002764, Grad1 Loss: 0.000007, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001672, LR: 0.001
    Current Lambdas: lambda1=0.2507, lambda2=0.2507, lambda3=0.2507
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  57%|█████▋    | 17/30 [5:49:16<2:49:07, 780.55s/it]Epoch: 17/30, Train Loss: 0.559337, MSE Loss: 0.002735, Grad1 Loss: 0.000006, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001810, LR: 0.001
    Current Lambdas: lambda1=0.2783, lambda2=0.2783, lambda3=0.2783
Training Epochs:  60%|██████    | 18/30 [6:01:34<2:33:35, 767.92s/it]Epoch: 18/30, Train Loss: 0.618173, MSE Loss: 0.002712, Grad1 Loss: 0.000006, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001704, LR: 0.001
    Current Lambdas: lambda1=0.3077, lambda2=0.3077, lambda3=0.3077
Training Epochs:  63%|██████▎   | 19/30 [6:13:54<2:19:14, 759.47s/it]Epoch: 19/30, Train Loss: 0.680575, MSE Loss: 0.002688, Grad1 Loss: 0.000005, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001695, LR: 0.001
    Current Lambdas: lambda1=0.3389, lambda2=0.3389, lambda3=0.3389
Training Epochs:  67%|██████▋   | 20/30 [6:25:49<2:04:20, 746.07s/it]Epoch: 20/30, Train Loss: 0.746544, MSE Loss: 0.002665, Grad1 Loss: 0.000005, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001675, LR: 0.001
    Current Lambdas: lambda1=0.3719, lambda2=0.3719, lambda3=0.3719
Training Epochs:  70%|███████   | 21/30 [6:38:05<1:51:26, 742.95s/it]Epoch: 21/30, Train Loss: 0.816083, MSE Loss: 0.002643, Grad1 Loss: 0.000005, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001626, LR: 0.001
    Current Lambdas: lambda1=0.4067, lambda2=0.4067, lambda3=0.4067
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  73%|███████▎  | 22/30 [6:50:19<1:38:41, 740.22s/it]Epoch: 22/30, Train Loss: 0.889191, MSE Loss: 0.002624, Grad1 Loss: 0.000005, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001594, LR: 0.001
    Current Lambdas: lambda1=0.4433, lambda2=0.4433, lambda3=0.4433
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  77%|███████▋  | 23/30 [7:02:35<1:26:14, 739.23s/it]Epoch: 23/30, Train Loss: 0.965863, MSE Loss: 0.002601, Grad1 Loss: 0.000004, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001503, LR: 0.001
    Current Lambdas: lambda1=0.4816, lambda2=0.4816, lambda3=0.4816
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  80%|████████  | 24/30 [7:14:34<1:13:18, 733.16s/it]Epoch: 24/30, Train Loss: 1.046108, MSE Loss: 0.002584, Grad1 Loss: 0.000004, Grad2 Loss: 0.000004, Grad4 Loss: 2.000000, Val Loss: 0.001531, LR: 0.001
    Current Lambdas: lambda1=0.5218, lambda2=0.5218, lambda3=0.5218
Training Epochs:  83%|████████▎ | 25/30 [7:26:41<1:00:55, 731.04s/it]Epoch: 25/30, Train Loss: 1.129921, MSE Loss: 0.002569, Grad1 Loss: 0.000004, Grad2 Loss: 0.000003, Grad4 Loss: 2.000000, Val Loss: 0.001483, LR: 0.001
    Current Lambdas: lambda1=0.5637, lambda2=0.5637, lambda3=0.5637
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  87%|████████▋ | 26/30 [7:38:37<48:26, 726.72s/it]  Epoch: 26/30, Train Loss: 1.217299, MSE Loss: 0.002550, Grad1 Loss: 0.000004, Grad2 Loss: 0.000003, Grad4 Loss: 2.000000, Val Loss: 0.001530, LR: 0.001
    Current Lambdas: lambda1=0.6074, lambda2=0.6074, lambda3=0.6074
Training Epochs:  90%|█████████ | 27/30 [7:50:37<36:13, 724.66s/it]Epoch: 27/30, Train Loss: 1.308251, MSE Loss: 0.002539, Grad1 Loss: 0.000004, Grad2 Loss: 0.000003, Grad4 Loss: 2.000000, Val Loss: 0.001389, LR: 0.001
    Current Lambdas: lambda1=0.6529, lambda2=0.6529, lambda3=0.6529
Best model saved to 2d_vanilla_05_8_custom.pth
Training Epochs:  93%|█████████▎| 28/30 [8:02:40<24:08, 724.08s/it]Epoch: 28/30, Train Loss: 1.402764, MSE Loss: 0.002521, Grad1 Loss: 0.000004, Grad2 Loss: 0.000003, Grad4 Loss: 2.000000, Val Loss: 0.001425, LR: 0.001
    Current Lambdas: lambda1=0.7001, lambda2=0.7001, lambda3=0.7001
Training Epochs:  97%|█████████▋| 29/30 [8:14:53<12:06, 726.93s/it]Epoch: 29/30, Train Loss: 1.500844, MSE Loss: 0.002503, Grad1 Loss: 0.000004, Grad2 Loss: 0.000003, Grad4 Loss: 2.000000, Val Loss: 0.001472, LR: 0.001
    Current Lambdas: lambda1=0.7492, lambda2=0.7492, lambda3=0.7492
Training Epochs: 100%|██████████| 30/30 [8:27:05<00:00, 728.35s/it]Training Epochs: 100%|██████████| 30/30 [8:27:05<00:00, 1014.18s/it]
Epoch: 30/30, Train Loss: 1.602496, MSE Loss: 0.002491, Grad1 Loss: 0.000004, Grad2 Loss: 0.000003, Grad4 Loss: 2.000000, Val Loss: 0.001404, LR: 0.001
    Current Lambdas: lambda1=0.8000, lambda2=0.8000, lambda3=0.8000
Best validation loss: 0.001389
Training completed. Best model saved as 2d_vanilla_05_8_custom.pth
