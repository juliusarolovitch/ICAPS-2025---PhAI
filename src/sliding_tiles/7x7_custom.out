/home/julius/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
Using device: cuda
Loaded normalization values from /home/julius/Desktop/icra_phai/src/sliding_tiles/normalization_values/vanilla_normalization_values_7x7.pkl
Loaded dataset from /home/julius/Desktop/icra_phai/src/sliding_tiles/datasets/vanilla_dataset_7x7.pkl
Training samples: 5831057, Validation samples: 1457765
Calculated input size for MLP: 149
Training MLPModel with learning type: priority
Training model with loss function: custom
Training Epochs:   0%|          | 0/30 [00:00<?, ?it/s]/home/julius/.local/lib/python3.8/site-packages/torch/autograd/graph.py:769: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Training Epochs:   3%|▎         | 1/30 [13:57<6:44:38, 837.19s/it]Epoch: 1/30, Train Loss: 0.004359, MSE Loss: 0.002798, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.031223, Val Loss: 0.001618, LR: 0.001000
    Current Lambdas: lambda1=0.0500, lambda2=0.0500, lambda3=0.0500
Best model saved to 7x7_custom.pth
Training Epochs:   7%|▋         | 2/30 [25:05<5:44:22, 737.95s/it]Epoch: 2/30, Train Loss: 0.002224, MSE Loss: 0.002126, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.001294, Val Loss: 0.001915, LR: 0.001000
    Current Lambdas: lambda1=0.0759, lambda2=0.0759, lambda3=0.0759
Training Epochs:  10%|█         | 3/30 [35:51<5:13:10, 695.94s/it]Epoch: 3/30, Train Loss: 0.002305, MSE Loss: 0.002045, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.002562, Val Loss: 0.001693, LR: 0.001000
    Current Lambdas: lambda1=0.1017, lambda2=0.1017, lambda3=0.1017
Training Epochs:  13%|█▎        | 4/30 [47:13<4:59:08, 690.31s/it]Epoch: 4/30, Train Loss: 0.002387, MSE Loss: 0.002000, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.003038, Val Loss: 0.001626, LR: 0.001000
    Current Lambdas: lambda1=0.1276, lambda2=0.1276, lambda3=0.1276
Training Epochs:  17%|█▋        | 5/30 [58:19<4:44:04, 681.78s/it]Epoch: 5/30, Train Loss: 0.002366, MSE Loss: 0.001972, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.002572, Val Loss: 0.001659, LR: 0.001000
    Current Lambdas: lambda1=0.1534, lambda2=0.1534, lambda3=0.1534
Training Epochs:  20%|██        | 6/30 [1:19:53<5:55:56, 889.87s/it]Epoch: 6/30, Train Loss: 0.002452, MSE Loss: 0.001964, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.002722, Val Loss: 0.001749, LR: 0.001000
    Current Lambdas: lambda1=0.1793, lambda2=0.1793, lambda3=0.1793
Training Epochs:  23%|██▎       | 7/30 [1:52:26<7:54:19, 1237.38s/it]Epoch: 7/30, Train Loss: 0.002637, MSE Loss: 0.001960, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.003303, Val Loss: 0.002792, LR: 0.001000
    Current Lambdas: lambda1=0.2052, lambda2=0.2052, lambda3=0.2052
Training Epochs:  27%|██▋       | 8/30 [2:24:35<8:54:25, 1457.53s/it]Epoch: 8/30, Train Loss: 0.002103, MSE Loss: 0.001794, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.001337, Val Loss: 0.001656, LR: 0.000500
    Current Lambdas: lambda1=0.2310, lambda2=0.2310, lambda3=0.2310
Training Epochs:  30%|███       | 9/30 [2:57:28<9:26:30, 1618.62s/it]Epoch: 9/30, Train Loss: 0.002039, MSE Loss: 0.001795, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.000950, Val Loss: 0.001771, LR: 0.000500
    Current Lambdas: lambda1=0.2569, lambda2=0.2569, lambda3=0.2569
Training Epochs:  33%|███▎      | 10/30 [3:29:25<9:30:16, 1710.85s/it]Epoch: 10/30, Train Loss: 0.002039, MSE Loss: 0.001799, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.000847, Val Loss: 0.001744, LR: 0.000500
    Current Lambdas: lambda1=0.2828, lambda2=0.2828, lambda3=0.2828
Training Epochs:  33%|███▎      | 10/30 [3:59:54<7:59:49, 1439.46s/it]
Epoch: 11/30, Train Loss: 0.002043, MSE Loss: 0.001799, Grad1 Loss: 0.000000, Grad2 Loss: 0.000000, Grad4 Loss: 0.000789, Val Loss: 0.001674, LR: 0.000500
    Current Lambdas: lambda1=0.3086, lambda2=0.3086, lambda3=0.3086
Early stopping triggered after 11 epochs.
Best validation loss: 0.001618
Training completed. Best model saved as 7x7_custom.pth
